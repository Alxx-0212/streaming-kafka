{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import time \n",
    "\n",
    "from kafka import KafkaProducer\n",
    "\n",
    "def json_serializer(data):\n",
    "    return json.dumps(data).encode('utf-8')\n",
    "\n",
    "server = 'localhost:9092'\n",
    "\n",
    "producer = KafkaProducer(\n",
    "    bootstrap_servers=[server],\n",
    "    value_serializer=json_serializer\n",
    ")\n",
    "\n",
    "producer.bootstrap_connected()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# t0 = time.time()\n",
    "\n",
    "# topic_name = 'test-topic'\n",
    "\n",
    "# tsend_start = time.time()\n",
    "# for i in range(10):\n",
    "#     message = {'number': i}\n",
    "#     producer.send(topic_name, value=message)\n",
    "#     print(f\"Sent: {message}\")\n",
    "#     time.sleep(0.05)\n",
    "# tsend_end = time.time()\n",
    "\n",
    "# tflush_start = time.time()\n",
    "# producer.flush()\n",
    "# tflush_end = time.time()\n",
    "\n",
    "# t1 = time.time()\n",
    "# print(f'took {(t1 - t0):.2f} seconds')\n",
    "# print(f'{(tsend_end-tsend_start)} seconds sending time.')\n",
    "# print(f'{(tflush_end-tflush_start)} seconds flushing time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No next row.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_59427/571853930.py:8: DtypeWarning: Columns (3) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  data = next(iterator)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def csv_reader(url : str) -> pd.DataFrame :\n",
    "    iterator = pd.read_csv(url, iterator=True, chunksize=100000, sep=',', compression='gzip')\n",
    "    df = pd.DataFrame()\n",
    "    try:\n",
    "        while True:\n",
    "            data = next(iterator)\n",
    "            if len(df)==0:\n",
    "                df=data\n",
    "            else:\n",
    "                df=pd.concat([df,data])\n",
    "            \n",
    "    except StopIteration:\n",
    "        print('No next row.')\n",
    "\n",
    "    return df\n",
    "\n",
    "def load_data_from_api(*args, **kwargs):\n",
    "    \"\"\"\n",
    "    Template for loading data from API\n",
    "    \"\"\"\n",
    "    # data_types = {\n",
    "    #     'VendorID': pd.Int64Dtype(),\n",
    "    #     'store_and_fwd_flag': str,\n",
    "    #     'RatecodeID': pd.Int64Dtype(),\n",
    "    #     'PULocationID': pd.Int64Dtype(),\n",
    "    #     'DOLocationID': pd.Int64Dtype(),\n",
    "    #     'passenger_count': pd.Int64Dtype(),\n",
    "    #     'trip_distance': float,\n",
    "    #     'fare_amount': float,\n",
    "    #     'extra': float,\n",
    "    #     'mta_tax': float,\n",
    "    #     'tip_amount': float,\n",
    "    #     'tolls_amount': float,\n",
    "    #     'ehail_fee': float,\n",
    "    #     'improvement_surcharge': float,\n",
    "    #     'total_amount': float,\n",
    "    #     'payment_type': pd.Int64Dtype(),\n",
    "    #     'trip_type': pd.Int64Dtype(),\n",
    "    #     'congestion_surcharge': float\n",
    "    # }\n",
    "    # parse_dates = ['lpep_pickup_datetime','lpep_dropoff_datetime']\n",
    "\n",
    "    url = 'https://github.com/DataTalksClub/nyc-tlc-data/releases/download/green/green_tripdata_2019-10.csv.gz'\n",
    "    frame = csv_reader(url)\n",
    "            \n",
    "                \n",
    "    return frame\n",
    "\n",
    "df = load_data_from_api()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 476386 entries, 0 to 476385\n",
      "Data columns (total 20 columns):\n",
      " #   Column                 Non-Null Count   Dtype  \n",
      "---  ------                 --------------   -----  \n",
      " 0   VendorID               387007 non-null  float64\n",
      " 1   lpep_pickup_datetime   476386 non-null  object \n",
      " 2   lpep_dropoff_datetime  476386 non-null  object \n",
      " 3   store_and_fwd_flag     387007 non-null  object \n",
      " 4   RatecodeID             387007 non-null  float64\n",
      " 5   PULocationID           476386 non-null  int64  \n",
      " 6   DOLocationID           476386 non-null  int64  \n",
      " 7   passenger_count        387007 non-null  float64\n",
      " 8   trip_distance          476386 non-null  float64\n",
      " 9   fare_amount            476386 non-null  float64\n",
      " 10  extra                  476386 non-null  float64\n",
      " 11  mta_tax                476386 non-null  float64\n",
      " 12  tip_amount             476386 non-null  float64\n",
      " 13  tolls_amount           476386 non-null  float64\n",
      " 14  ehail_fee              0 non-null       float64\n",
      " 15  improvement_surcharge  476386 non-null  float64\n",
      " 16  total_amount           476386 non-null  float64\n",
      " 17  payment_type           387007 non-null  float64\n",
      " 18  trip_type              387005 non-null  float64\n",
      " 19  congestion_surcharge   387007 non-null  float64\n",
      "dtypes: float64(15), int64(2), object(3)\n",
      "memory usage: 72.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "took 44.52 seconds\n",
      "40.28867030143738 seconds sending time.\n",
      "4.2281694412231445 seconds flushing time.\n"
     ]
    }
   ],
   "source": [
    "main_df = df[[\n",
    "    'lpep_pickup_datetime',\n",
    "    'lpep_dropoff_datetime',\n",
    "    'PULocationID',\n",
    "    'DOLocationID',\n",
    "    'passenger_count',\n",
    "    'trip_distance',\n",
    "    'tip_amount'\n",
    "    ]]\n",
    "\n",
    "topic_name = 'green-trips'\n",
    "tstart = time.time()\n",
    "tsend_start = time.time()\n",
    "for row in main_df.itertuples(index=False):\n",
    "    row_dict = {col: getattr(row, col) for col in row._fields}\n",
    "    producer.send(topic_name, value=row_dict)\n",
    "tsend_end = time.time()\n",
    "\n",
    "tflush_start = time.time()\n",
    "producer.flush()\n",
    "tflush_end = time.time()\n",
    "\n",
    "tend = time.time()\n",
    "print(f'took {(tend - tstart):.2f} seconds')\n",
    "print(f'{(tsend_end-tsend_start)} seconds sending time.')\n",
    "print(f'{(tflush_end-tflush_start)} seconds flushing time.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "your 131072x1 screen size is bogus. expect trouble\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/09 15:45:16 WARN Utils: Your hostname, LAPTOP-P1QAHOCG resolves to a loopback address: 127.0.1.1; using 172.26.185.65 instead (on interface eth0)\n",
      "24/04/09 15:45:16 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      ":: loading settings :: url = jar:file:/home/alexxgo21/spark-3.3.2-bin-hadoop3/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /home/alexxgo21/.ivy2/cache\n",
      "The jars for the packages stored in: /home/alexxgo21/.ivy2/jars\n",
      "org.apache.spark#spark-sql-kafka-0-10_2.12 added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-5cd26623-c57c-48b2-8a0c-553cbbfa6155;1.0\n",
      "\tconfs: [default]\n",
      "\tfound org.apache.spark#spark-sql-kafka-0-10_2.12;3.3.2 in central\n",
      "\tfound org.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.2 in central\n",
      "\tfound org.apache.kafka#kafka-clients;2.8.1 in central\n",
      "\tfound org.lz4#lz4-java;1.8.0 in central\n",
      "\tfound org.xerial.snappy#snappy-java;1.1.8.4 in central\n",
      "\tfound org.slf4j#slf4j-api;1.7.32 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-runtime;3.3.2 in central\n",
      "\tfound org.spark-project.spark#unused;1.0.0 in central\n",
      "\tfound org.apache.hadoop#hadoop-client-api;3.3.2 in central\n",
      "\tfound commons-logging#commons-logging;1.1.3 in central\n",
      "\tfound com.google.code.findbugs#jsr305;3.0.0 in central\n",
      "\tfound org.apache.commons#commons-pool2;2.11.1 in central\n",
      ":: resolution report :: resolve 376ms :: artifacts dl 9ms\n",
      "\t:: modules in use:\n",
      "\tcom.google.code.findbugs#jsr305;3.0.0 from central in [default]\n",
      "\tcommons-logging#commons-logging;1.1.3 from central in [default]\n",
      "\torg.apache.commons#commons-pool2;2.11.1 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-api;3.3.2 from central in [default]\n",
      "\torg.apache.hadoop#hadoop-client-runtime;3.3.2 from central in [default]\n",
      "\torg.apache.kafka#kafka-clients;2.8.1 from central in [default]\n",
      "\torg.apache.spark#spark-sql-kafka-0-10_2.12;3.3.2 from central in [default]\n",
      "\torg.apache.spark#spark-token-provider-kafka-0-10_2.12;3.3.2 from central in [default]\n",
      "\torg.lz4#lz4-java;1.8.0 from central in [default]\n",
      "\torg.slf4j#slf4j-api;1.7.32 from central in [default]\n",
      "\torg.spark-project.spark#unused;1.0.0 from central in [default]\n",
      "\torg.xerial.snappy#snappy-java;1.1.8.4 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   12  |   0   |   0   |   0   ||   12  |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-5cd26623-c57c-48b2-8a0c-553cbbfa6155\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 12 already retrieved (0kB/7ms)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/09 15:45:18 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "pyspark_version = pyspark.__version__\n",
    "kafka_jar_package = f\"org.apache.spark:spark-sql-kafka-0-10_2.12:{pyspark_version}\"\n",
    "\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .appName(\"GreenTripsConsumer\") \\\n",
    "    .config(\"spark.jars.packages\", kafka_jar_package) \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "green_stream = spark \\\n",
    "    .readStream \\\n",
    "    .format(\"kafka\") \\\n",
    "    .option(\"kafka.bootstrap.servers\", \"localhost:9092\") \\\n",
    "    .option(\"subscribe\", \"green-trips\") \\\n",
    "    .option(\"startingOffsets\", \"earliest\") \\\n",
    "    .load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/09 15:45:21 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-5061116f-b0ce-4427-b09a-d34c48c132c0. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/09 15:45:21 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(key=None, value=bytearray(b'{\"lpep_pickup_datetime\": \"2019-10-01 00:26:02\", \"lpep_dropoff_datetime\": \"2019-10-01 00:39:58\", \"PULocationID\": 112, \"DOLocationID\": 196, \"passenger_count\": 1.0, \"trip_distance\": 5.88, \"tip_amount\": 0.0}'), topic='green-trips', partition=0, offset=0, timestamp=datetime.datetime(2024, 4, 9, 15, 44, 30, 635000), timestampType=0)]\n"
     ]
    }
   ],
   "source": [
    "def peek(mini_batch, batch_id):\n",
    "    first_row = mini_batch.take(1)\n",
    "\n",
    "    if first_row:\n",
    "        print(first_row)\n",
    "\n",
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import types\n",
    "\n",
    "schema = types.StructType() \\\n",
    "    .add('lpep_pickup_datetime', types.StringType()) \\\n",
    "    .add('lpep_dropoff_datetime', types.StringType()) \\\n",
    "    .add('PULocationID', types.IntegerType()) \\\n",
    "    .add('DOLocationID', types.IntegerType()) \\\n",
    "    .add('passenger_count', types.DoubleType()) \\\n",
    "    .add('trip_distance', types.DoubleType()) \\\n",
    "    .add('tip_amount', types.DoubleType())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "def debug_and_process(batch_df, batch_id):\n",
    "    batch_df.select(F.col(\"value\").cast('STRING')).show(truncate=False)\n",
    "    parsed_batch = batch_df.select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "    .select(\"data.*\")\n",
    "    \n",
    "    print(f\"Batch ID: {batch_id}\")\n",
    "    parsed_batch.show(truncate=False)\n",
    "\n",
    "green_stream = green_stream \\\n",
    "  .select(F.from_json(F.col(\"value\").cast('STRING'), schema).alias(\"data\")) \\\n",
    "  .select(\"data.*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24/04/09 15:45:41 WARN ResolveWriteToStream: Temporary checkpoint location created which is deleted normally when the query didn't fail: /tmp/temporary-73672a6f-2700-4d4e-907f-9150281bb2ff. If it's required to delete it under any circumstances, please set spark.sql.streaming.forceDeleteTempCheckpointLocation to true. Important to know deleting temp checkpoint folder is best effort.\n",
      "24/04/09 15:45:41 WARN ResolveWriteToStream: spark.sql.adaptive.enabled is not supported in streaming DataFrames/Datasets and will be disabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(lpep_pickup_datetime='2019-10-01 00:26:02', lpep_dropoff_datetime='2019-10-01 00:39:58', PULocationID=112, DOLocationID=196, passenger_count=1.0, trip_distance=5.88, tip_amount=0.0)]\n"
     ]
    }
   ],
   "source": [
    "query = green_stream.writeStream.foreachBatch(peek).start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "query.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 3:========================>                                 (5 + 7) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------------+-----+\n",
      "|              window|DOLocationID|count|\n",
      "+--------------------+------------+-----+\n",
      "|{2024-04-09 15:45...|          74|17741|\n",
      "|{2024-04-09 15:45...|          42|15942|\n",
      "|{2024-04-09 15:45...|          41|14061|\n",
      "|{2024-04-09 15:45...|          75|12840|\n",
      "|{2024-04-09 15:45...|         129|11930|\n",
      "|{2024-04-09 15:45...|           7|11533|\n",
      "|{2024-04-09 15:45...|         166|10845|\n",
      "|{2024-04-09 15:45...|         236| 7913|\n",
      "|{2024-04-09 15:45...|         223| 7542|\n",
      "|{2024-04-09 15:45...|         238| 7318|\n",
      "|{2024-04-09 15:45...|          82| 7292|\n",
      "|{2024-04-09 15:45...|         181| 7282|\n",
      "|{2024-04-09 15:45...|          95| 7244|\n",
      "|{2024-04-09 15:45...|         244| 6733|\n",
      "|{2024-04-09 15:45...|          61| 6606|\n",
      "|{2024-04-09 15:45...|         116| 6339|\n",
      "|{2024-04-09 15:45...|         138| 6144|\n",
      "|{2024-04-09 15:45...|          97| 6050|\n",
      "|{2024-04-09 15:45...|          49| 5221|\n",
      "|{2024-04-09 15:45...|         151| 5153|\n",
      "+--------------------+------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import current_timestamp, window, count\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Create a Spark session\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Read the green csv file\n",
    "df_green = spark.read.option(\"header\", True).csv(\"./green_tripdata_2019-10.csv\")\n",
    "\n",
    "# Add the timestamp column\n",
    "df_green_with_timestamp = df_green.withColumn(\"timestamp\", current_timestamp())\n",
    "\n",
    "# Define a window specification\n",
    "window_spec = window(df_green_with_timestamp[\"timestamp\"], \"5 minutes\")\n",
    "\n",
    "# Group by the window specification and \"DOLocationID\", then count the number of occurrences in each group\n",
    "windowed_df = df_green_with_timestamp.groupBy(\n",
    "    window_spec,\n",
    "    df_green_with_timestamp[\"DOLocationID\"]\n",
    ").count()\n",
    "\n",
    "# Order by count in descending order\n",
    "windowed_df = windowed_df.orderBy(windowed_df[\"count\"].desc())\n",
    "\n",
    "# Show the result\n",
    "windowed_df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf-gpu",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
